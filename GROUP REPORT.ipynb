{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72285a01-7d31-423c-a18f-b5b57b8c0bf0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://leverageedublog.s3.ap-south-1.amazonaws.com/blog/wp-content/uploads/2020/03/24185535/Online-Learning.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"https://blog.edmentum.com/sites/blog.edmentum.com/files/infographics/Four_Layers_0.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"https://i0.wp.com/content.edupristine.com/images/blogs/CFA-RESULT-2016.jpg?w=525&ssl=1\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ddc3f-f12e-4994-8d39-b20ee7d0f271",
   "metadata": {},
   "source": [
    "Improvement in technology has changed how human beings live in the past decades. Technology has improved the living standard of humans, as well as the quality of education in large educational institutions. At UBC, faculty members utilize technology to enhance students’ learning experience, learning websites such as iClicker and Webwork are frequently used by professors and students in order to solidify the knowledge absorbed by students in the lectures. \n",
    "\n",
    "The dataset that is used in this project is from https://archive.ics.uci.edu/ml/datasets/User%2BKnowledge%2BModeling.\n",
    "The dataset records the students' knowledge status about the subject of Electrical DC Machines at Gazi University, Turkey in 2009. Gazi University has developed the user modelling system which keeps track of students’ learning activities on the web learning environment and stores the data in the model. The dataset allows us to observe students’ learning experiences on the online learning platform and exam performance. Then the data are used as input data by a user modelling algorithm aimed at creating a consistent description of the model of students. Eventually, the input data is used to create and update the user knowledge model. The user knowledge model would be used to improve the web learning environment in order to help students learn much better. \n",
    "\n",
    "In our project, we would like to utilize the dataset and classify the knowledge level of users (UNS) based on the information provided in the dataset. \n",
    "\n",
    "The dataset contains 6 columns and a total of 3 sheets in the excel file (dataset):\n",
    "\n",
    "STG: degree of study time for goal object materials <br>\n",
    "SCG: degree of repetition number of user for goal object materials <br>\n",
    "STR: degree of study time of user for related objects with goal object <br>\n",
    "LPR: exam performance of user for related objects with goal object <br>\n",
    "PEG: exam performance of user for goal objects <br>\n",
    "UNS: the knowledge level of user <br>\n",
    "\n",
    "Sheet 1 is only the introduction of the dataset, sheet 2 is the training dataset and sheet 3 is the testing dataset. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391f7df-ffe6-4e0f-a52b-2e7113293dbd",
   "metadata": {},
   "source": [
    "## Methods & Results\n",
    "### Overview of our Methodology \n",
    "The User Knowledge Modeling dataset has provided us the training dataset and testing dataset already. Here is an overview of our data analysis methodology (detailed explanation will be presented at each step):\n",
    "\n",
    "***Reading & Wrangling***: Skipped as the dataset has been split into training dataset and testing dataset already. Overall, the both training and testing datasets are tidy. <br>\n",
    "<br>\n",
    "***Train/Test split***: spliting the tidy dataframe into a training set and testing set, then summarizing the sample statistics of the training data.<br>\n",
    "<br>\n",
    "***Predictor variables selection***: find the combination of predictors that yield the relatively highest prediction accuracy, by using forward selection. <br>\n",
    "<br>\n",
    "***K-Nearest Neighbor classification***: model tuning with cross-validation, reconfigure and re-train the model with the best K parameter, predict the test set for final model accuracy validation.\n",
    "Results and Conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc3b0a-21c0-4d21-afb4-a67cadf92bcc",
   "metadata": {},
   "source": [
    "### Loading Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b1eee1-b28e-4ca2-bd3d-6a916c8f5ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.6     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.7     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.9\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.2     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "── \u001b[1mAttaching packages\u001b[22m ────────────────────────────────────── tidymodels 1.0.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mbroom       \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mrsample     \u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdials       \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mtune        \u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34minfer       \u001b[39m 1.0.2     \u001b[32m✔\u001b[39m \u001b[34mworkflows   \u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mmodeldata   \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mworkflowsets\u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mparsnip     \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34myardstick   \u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mrecipes     \u001b[39m 1.0.1     \n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m   masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m  masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m      masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m   masks \u001b[34mstats\u001b[39m::step()\n",
      "\u001b[34m•\u001b[39m Use suppressPackageStartupMessages() to eliminate package startup messages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tidyverse) # to perform operations on the data set\n",
    "library(readxl) # To read the dataset into R\n",
    "library(tidymodels)\n",
    "library(forcats)\n",
    "library(RColorBrewer)\n",
    "library(repr) # to set graph plot size and performing some operations on data set\n",
    "library(ggplot2)\n",
    "library(knitr)\n",
    "options(repr.matrix.max.rows = 6) # limits output of dataframes to 6 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd92cf-2f53-4016-833a-7dd31fb4e8e2",
   "metadata": {},
   "source": [
    "### Reading and Wrangling our dataset from the web into R\n",
    "* We downloaded the file from the web using a URL so that it was reproducible and read the data file\n",
    "* Due to there being several sheets in a single excel file, we read them separately and merged them into a single dataframe called “User_Knowledge”. \n",
    "* The dataset in Sheet 1 (**Information**) has only metadata, so we will ignore this sheet.\n",
    "* The dataset in Sheet 2 (**Training_Data**) has 1 column of metadata and a total of 258 rows. Therefore `select ()` is used as the argument.\n",
    "* The dataset in Sheet 3 (**Test_Data**) has 1 column of metadata and a total of 145 rows. Therefore `select ()` is used as the argument.\n",
    "* There was no missing value in any rows.\n",
    "* The two given datasets have slightly different UNS subcategories, where the **Training_Data** has categories “*High*,” “*Medium*,” “*Low*,” and “*very_low*,” whereas the **Test_Data** has a “*Very Low*” class instead. Therefore, we change every “*very_low*” to “*Very Low*”.\n",
    "* We combined the rows of both datasets to form one dataset using `rbind()` because in our proposal submission, we used these two sets as given to us by the dataset and later ran into problems in the analysis.\n",
    "* The UNS column will be our target variable for this analysis. Therefore, it was converted into a factor variable using `as_factor()`.\n",
    "* The resulting dataset is our wrangled `User_knowledge` dataset with **403** valuable observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b892ea7a-9981-4b49-94e6-d2ecde2847af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mNew names:\n",
      "\u001b[36m•\u001b[39m `` -> `...7`\n",
      "\u001b[36m•\u001b[39m `` -> `...8`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 258 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>Very Low</td></tr>\n",
       "\t<tr><td>0.08</td><td>0.08</td><td>0.10</td><td>0.24</td><td>0.90</td><td>High    </td></tr>\n",
       "\t<tr><td>0.06</td><td>0.06</td><td>0.05</td><td>0.25</td><td>0.33</td><td>Low     </td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.54</td><td>0.82</td><td>0.71</td><td>0.29</td><td>0.77</td><td>High  </td></tr>\n",
       "\t<tr><td>0.50</td><td>0.75</td><td>0.81</td><td>0.61</td><td>0.26</td><td>Middle</td></tr>\n",
       "\t<tr><td>0.66</td><td>0.90</td><td>0.76</td><td>0.87</td><td>0.74</td><td>High  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 258 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & Very Low\\\\\n",
       "\t 0.08 & 0.08 & 0.10 & 0.24 & 0.90 & High    \\\\\n",
       "\t 0.06 & 0.06 & 0.05 & 0.25 & 0.33 & Low     \\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 0.54 & 0.82 & 0.71 & 0.29 & 0.77 & High  \\\\\n",
       "\t 0.50 & 0.75 & 0.81 & 0.61 & 0.26 & Middle\\\\\n",
       "\t 0.66 & 0.90 & 0.76 & 0.87 & 0.74 & High  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 258 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | Very Low |\n",
       "| 0.08 | 0.08 | 0.10 | 0.24 | 0.90 | High     |\n",
       "| 0.06 | 0.06 | 0.05 | 0.25 | 0.33 | Low      |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 0.54 | 0.82 | 0.71 | 0.29 | 0.77 | High   |\n",
       "| 0.50 | 0.75 | 0.81 | 0.61 | 0.26 | Middle |\n",
       "| 0.66 | 0.90 | 0.76 | 0.87 | 0.74 | High   |\n",
       "\n"
      ],
      "text/plain": [
       "    STG  SCG  STR  LPR  PEG  UNS     \n",
       "1   0.00 0.00 0.00 0.00 0.00 Very Low\n",
       "2   0.08 0.08 0.10 0.24 0.90 High    \n",
       "3   0.06 0.06 0.05 0.25 0.33 Low     \n",
       "⋮   ⋮    ⋮    ⋮    ⋮    ⋮    ⋮       \n",
       "256 0.54 0.82 0.71 0.29 0.77 High    \n",
       "257 0.50 0.75 0.81 0.61 0.26 Middle  \n",
       "258 0.66 0.90 0.76 0.87 0.74 High    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reading the training data set from the Web into R and cleaning it\n",
    "user_data_training <- download.file(url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00257/Data_User_Modeling_Dataset_Hamdi%20Tolga%20KAHRAMAN.xls\", destfile = \"user_data.xls\")\n",
    "user_data_training <- read_excel(\"user_data.xls\", sheet = 2) # Selecting the \"Training_Data\"\n",
    "\n",
    "user_data_training <- select(user_data_training, STG:UNS) #Selecting only the required columns (not including metadata)\n",
    "                    \n",
    "user_data_training$UNS[user_data_training$UNS == \"very_low\"] <- \"Very Low\" #Setting the \"very_low field to Very Low to match with Test_Data\"\n",
    "user_data_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8094c7bd-e485-451b-a17b-ae90158d02ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mNew names:\n",
      "\u001b[36m•\u001b[39m `` -> `...7`\n",
      "\u001b[36m•\u001b[39m `` -> `...8`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 145 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>0.10</td><td>0.50</td><td>0.26</td><td>0.05</td><td>Very Low</td></tr>\n",
       "\t<tr><td>0.05</td><td>0.05</td><td>0.55</td><td>0.60</td><td>0.14</td><td>Low     </td></tr>\n",
       "\t<tr><td>0.08</td><td>0.18</td><td>0.63</td><td>0.60</td><td>0.85</td><td>High    </td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.56</td><td>0.60</td><td>0.77</td><td>0.13</td><td>0.32</td><td>Low   </td></tr>\n",
       "\t<tr><td>0.66</td><td>0.68</td><td>0.81</td><td>0.57</td><td>0.57</td><td>Middle</td></tr>\n",
       "\t<tr><td>0.68</td><td>0.64</td><td>0.79</td><td>0.97</td><td>0.24</td><td>Middle</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 145 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 0.10 & 0.50 & 0.26 & 0.05 & Very Low\\\\\n",
       "\t 0.05 & 0.05 & 0.55 & 0.60 & 0.14 & Low     \\\\\n",
       "\t 0.08 & 0.18 & 0.63 & 0.60 & 0.85 & High    \\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 0.56 & 0.60 & 0.77 & 0.13 & 0.32 & Low   \\\\\n",
       "\t 0.66 & 0.68 & 0.81 & 0.57 & 0.57 & Middle\\\\\n",
       "\t 0.68 & 0.64 & 0.79 & 0.97 & 0.24 & Middle\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 145 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.00 | 0.10 | 0.50 | 0.26 | 0.05 | Very Low |\n",
       "| 0.05 | 0.05 | 0.55 | 0.60 | 0.14 | Low      |\n",
       "| 0.08 | 0.18 | 0.63 | 0.60 | 0.85 | High     |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 0.56 | 0.60 | 0.77 | 0.13 | 0.32 | Low    |\n",
       "| 0.66 | 0.68 | 0.81 | 0.57 | 0.57 | Middle |\n",
       "| 0.68 | 0.64 | 0.79 | 0.97 | 0.24 | Middle |\n",
       "\n"
      ],
      "text/plain": [
       "    STG  SCG  STR  LPR  PEG  UNS     \n",
       "1   0.00 0.10 0.50 0.26 0.05 Very Low\n",
       "2   0.05 0.05 0.55 0.60 0.14 Low     \n",
       "3   0.08 0.18 0.63 0.60 0.85 High    \n",
       "⋮   ⋮    ⋮    ⋮    ⋮    ⋮    ⋮       \n",
       "143 0.56 0.60 0.77 0.13 0.32 Low     \n",
       "144 0.66 0.68 0.81 0.57 0.57 Middle  \n",
       "145 0.68 0.64 0.79 0.97 0.24 Middle  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reading the testing data set from the Web into R and cleaning it\n",
    "user_data_test <- read_excel(\"user_data.xls\", sheet = 3) #selecting the \"Test_Data\"\n",
    "user_data_test <- select(user_data_test, STG:UNS) #Selecting only the required columns (not including metadata)\n",
    "                 \n",
    "user_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de5a2c56-77c0-417b-afec-f7bccdd893ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 403 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>Very Low</td></tr>\n",
       "\t<tr><td>0.08</td><td>0.08</td><td>0.10</td><td>0.24</td><td>0.90</td><td>High    </td></tr>\n",
       "\t<tr><td>0.06</td><td>0.06</td><td>0.05</td><td>0.25</td><td>0.33</td><td>Low     </td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.56</td><td>0.60</td><td>0.77</td><td>0.13</td><td>0.32</td><td>Low   </td></tr>\n",
       "\t<tr><td>0.66</td><td>0.68</td><td>0.81</td><td>0.57</td><td>0.57</td><td>Middle</td></tr>\n",
       "\t<tr><td>0.68</td><td>0.64</td><td>0.79</td><td>0.97</td><td>0.24</td><td>Middle</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 403 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & Very Low\\\\\n",
       "\t 0.08 & 0.08 & 0.10 & 0.24 & 0.90 & High    \\\\\n",
       "\t 0.06 & 0.06 & 0.05 & 0.25 & 0.33 & Low     \\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 0.56 & 0.60 & 0.77 & 0.13 & 0.32 & Low   \\\\\n",
       "\t 0.66 & 0.68 & 0.81 & 0.57 & 0.57 & Middle\\\\\n",
       "\t 0.68 & 0.64 & 0.79 & 0.97 & 0.24 & Middle\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 403 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | Very Low |\n",
       "| 0.08 | 0.08 | 0.10 | 0.24 | 0.90 | High     |\n",
       "| 0.06 | 0.06 | 0.05 | 0.25 | 0.33 | Low      |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 0.56 | 0.60 | 0.77 | 0.13 | 0.32 | Low    |\n",
       "| 0.66 | 0.68 | 0.81 | 0.57 | 0.57 | Middle |\n",
       "| 0.68 | 0.64 | 0.79 | 0.97 | 0.24 | Middle |\n",
       "\n"
      ],
      "text/plain": [
       "    STG  SCG  STR  LPR  PEG  UNS     \n",
       "1   0.00 0.00 0.00 0.00 0.00 Very Low\n",
       "2   0.08 0.08 0.10 0.24 0.90 High    \n",
       "3   0.06 0.06 0.05 0.25 0.33 Low     \n",
       "⋮   ⋮    ⋮    ⋮    ⋮    ⋮    ⋮       \n",
       "401 0.56 0.60 0.77 0.13 0.32 Low     \n",
       "402 0.66 0.68 0.81 0.57 0.57 Middle  \n",
       "403 0.68 0.64 0.79 0.97 0.24 Middle  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Combining both user and test data sets (they have been taken from separate excel sheets)\n",
    "#robk@statmethods.net, Robert Kabacoff -. “Merging Data.” Quick-R: Merging, www.statmethods.net/management/merging.html.\n",
    "User_Knowledge <- rbind(user_data_training, user_data_test)|>  #binidng rows in both the data frames using rbind\n",
    "         mutate(UNS = as_factor(UNS))# mutating UNS column to be of type factor since that column needs to be predicted\n",
    "\n",
    "User_Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6cbb4-c3db-42b3-9cc4-9e3496ef449a",
   "metadata": {},
   "source": [
    "### Summarizing the Training Data\n",
    "* Based on our data wrangling step, we have a total of **403** usable observations.\n",
    "* Thus, we have done a random split of **0.70**, which means 70% of the data is split into the training set and the remaining 30% into the testing set to ensure that we have an adequate amount of observations for training and evaluating the prediction accuracy of our model.\n",
    "* We also used `set.seed()` for reproducible results as `initial_split()` randomly samples from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9f1f0d-7d29-4e0a-b18e-4629c3cc7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1) # Use of set.seed to make the split reproducible\n",
    "User_Knowledge_split <- initial_split(User_Knowledge, prop = 0.7, strata = UNS) #doing the decided 70-30 split of our combined data set\n",
    "user_knowledge_training <- training(User_Knowledge_split)\n",
    "user_knowledge_test <- testing(User_Knowledge_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b98fa2-b84c-4958-9eac-3a5cd3800523",
   "metadata": {},
   "source": [
    "* Now we begin exploratory data analysis by summarizing the data into different types of tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f74a0f0-29fa-48e1-8b74-b9197ef3ea99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>281</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 1\n",
       "\\begin{tabular}{l}\n",
       " n\\\\\n",
       " <int>\\\\\n",
       "\\hline\n",
       "\t 281\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 1\n",
       "\n",
       "| n &lt;int&gt; |\n",
       "|---|\n",
       "| 281 |\n",
       "\n"
      ],
      "text/plain": [
       "  n  \n",
       "1 281"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>122</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 1\n",
       "\\begin{tabular}{l}\n",
       " n\\\\\n",
       " <int>\\\\\n",
       "\\hline\n",
       "\t 122\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 1\n",
       "\n",
       "| n &lt;int&gt; |\n",
       "|---|\n",
       "| 122 |\n",
       "\n"
      ],
      "text/plain": [
       "  n  \n",
       "1 122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Counting the total number of rows in training data set\n",
    "training_count <- count(user_knowledge_training) \n",
    "training_count\n",
    "\n",
    "#Counting the total number of rows in test data set\n",
    "test_count <- count(user_knowledge_test) \n",
    "test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e94a4-50ab-4b9f-b46c-f11bb0b77e88",
   "metadata": {},
   "source": [
    "* The tables report that there is **281** observations in **user_knowledge_training** dataset and **122** observations in **user_knowledge_test** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677c17c7-ae71-43c1-8cd7-fd80e44941ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 4 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>UNS</th><th scope=col>Number_Of_Observations</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Very Low</td><td>35</td></tr>\n",
       "\t<tr><td>High    </td><td>71</td></tr>\n",
       "\t<tr><td>Low     </td><td>90</td></tr>\n",
       "\t<tr><td>Middle  </td><td>85</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 4 × 2\n",
       "\\begin{tabular}{ll}\n",
       " UNS & Number\\_Of\\_Observations\\\\\n",
       " <fct> & <int>\\\\\n",
       "\\hline\n",
       "\t Very Low & 35\\\\\n",
       "\t High     & 71\\\\\n",
       "\t Low      & 90\\\\\n",
       "\t Middle   & 85\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 4 × 2\n",
       "\n",
       "| UNS &lt;fct&gt; | Number_Of_Observations &lt;int&gt; |\n",
       "|---|---|\n",
       "| Very Low | 35 |\n",
       "| High     | 71 |\n",
       "| Low      | 90 |\n",
       "| Middle   | 85 |\n",
       "\n"
      ],
      "text/plain": [
       "  UNS      Number_Of_Observations\n",
       "1 Very Low 35                    \n",
       "2 High     71                    \n",
       "3 Low      90                    \n",
       "4 Middle   85                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Counting the total number of observations of each class (training data set) to check for class imbalance (if any)\n",
    "user_data_summarize <- user_knowledge_training |>\n",
    "                       group_by(UNS) |>\n",
    "                       summarize(Number_Of_Observations = n())\n",
    "user_data_summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd288b5-8381-4800-bce0-56d6904c3c11",
   "metadata": {},
   "source": [
    "* The table reports the number of observations in each class.\n",
    "* Using `group_by(UNS)`, we grouped our table by the UNS variable and then applied `n()` to count the number of observations in each class.\n",
    "* It is observed that there are fewer cases of *\"Very Low\"* as compared to the other classes in our training dataset, but there is no need to over-sample the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb204b0-9966-4343-a992-3dc18b157861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.3460427</td><td>0.3679537</td><td>0.4660676</td><td>0.4365516</td><td>0.4583737</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " STG & SCG & STR & LPR & PEG\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.3460427 & 0.3679537 & 0.4660676 & 0.4365516 & 0.4583737\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 5\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 0.3460427 | 0.3679537 | 0.4660676 | 0.4365516 | 0.4583737 |\n",
       "\n"
      ],
      "text/plain": [
       "  STG       SCG       STR       LPR       PEG      \n",
       "1 0.3460427 0.3679537 0.4660676 0.4365516 0.4583737"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using map_df to calculate mean of each of our predictor variables and return the result as a dataframe\n",
    "user_data_mean <- select(user_knowledge_training, STG:PEG) |>\n",
    "                  map_df(mean)\n",
    "user_data_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c568a-e2f7-45d9-95bf-7b5ee23076b7",
   "metadata": {},
   "source": [
    "* The table reports the means of the predictor variables.\n",
    "* Here, we select all the predictor variables and apply `map_df` to apply `mean()` to each of the columns to obtain the mean statistic for each variable.\n",
    "* The table does not suggest that any specific predictor needs more attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9065eb-2345-4558-8fc1-905549a80a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.2428</td><td>0.274</td><td>0.3417143</td><td>0.2903143</td><td>0.09714286</td><td> Very Low</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.2428 & 0.274 & 0.3417143 & 0.2903143 & 0.09714286 &  Very Low\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.2428 | 0.274 | 0.3417143 | 0.2903143 | 0.09714286 |  Very Low |\n",
       "\n"
      ],
      "text/plain": [
       "  STG    SCG   STR       LPR       PEG        UNS      \n",
       "1 0.2428 0.274 0.3417143 0.2903143 0.09714286  Very Low"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.3194</td><td>0.3257667</td><td>0.4515556</td><td>0.4451111</td><td>0.2571444</td><td>Low</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.3194 & 0.3257667 & 0.4515556 & 0.4451111 & 0.2571444 & Low\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.3194 | 0.3257667 | 0.4515556 | 0.4451111 | 0.2571444 | Low |\n",
       "\n"
      ],
      "text/plain": [
       "  STG    SCG       STR       LPR       PEG       UNS\n",
       "1 0.3194 0.3257667 0.4515556 0.4451111 0.2571444 Low"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.3612118</td><td>0.3835294</td><td>0.5049412</td><td>0.3923529</td><td>0.528</td><td> Middle</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.3612118 & 0.3835294 & 0.5049412 & 0.3923529 & 0.528 &  Middle\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.3612118 | 0.3835294 | 0.5049412 | 0.3923529 | 0.528 |  Middle |\n",
       "\n"
      ],
      "text/plain": [
       "  STG       SCG       STR       LPR       PEG   UNS    \n",
       "1 0.3612118 0.3835294 0.5049412 0.3923529 0.528  Middle"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.4125493</td><td>0.4490986</td><td>0.4992254</td><td>0.5507042</td><td>0.808169</td><td>High</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.4125493 & 0.4490986 & 0.4992254 & 0.5507042 & 0.808169 & High\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.4125493 | 0.4490986 | 0.4992254 | 0.5507042 | 0.808169 | High |\n",
       "\n"
      ],
      "text/plain": [
       "  STG       SCG       STR       LPR       PEG      UNS \n",
       "1 0.4125493 0.4490986 0.4992254 0.5507042 0.808169 High"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using map_df to calculate mean of each of our predictor variables for each class and return the result as a dataframe\n",
    "\n",
    "verylow_avg_pred_vals <- user_knowledge_training |> # Mean values of predictors for Very Low UNS\n",
    "    filter(UNS == \"Very Low\") |>\n",
    "    select( - UNS) |>\n",
    "    map_df(mean) |>\n",
    "    mutate(UNS =c(\" Very Low\"))\n",
    "verylow_avg_pred_vals \n",
    "    \n",
    "low_avg_pred_vals <- user_knowledge_training |>  # Mean values of predictors for UNS of Low UNS\n",
    "    filter(UNS == \"Low\") |>\n",
    "    select( - UNS) |>\n",
    "    map_df(mean) |>\n",
    "    mutate(UNS =c(\"Low\"))\n",
    "low_avg_pred_vals\n",
    "\n",
    "middle_avg_pred_vals <- user_knowledge_training |> # Mean values of predictors for UNS of Middle UNS\n",
    "    filter(UNS == \"Middle\") |>\n",
    "    select( - UNS) |>\n",
    "    map_df(mean) |>\n",
    "    mutate(UNS =c(\" Middle\"))\n",
    "middle_avg_pred_vals\n",
    "\n",
    "high_avg_pred_vals <- user_knowledge_training |> # Mean values of predictors for UNS of High UNS\n",
    "    filter(UNS == \"High\") |>\n",
    "    select( - UNS) |>\n",
    "    map_df(mean) |>\n",
    "    mutate(UNS =c(\"High\"))\n",
    "high_avg_pred_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ad3d1-f9b7-46a5-a9c2-3b0a0ae7d4f1",
   "metadata": {},
   "source": [
    "* Tables above report the means of the predictor variables for each knowledge level.\n",
    "* The following tables help us to compare the summary statistics for each knowledge level.\n",
    "* It is observed that the mean value of **LPR** in Low is higher than that in Middle. Otherwise, no apparent issues in these statistics between the four classes that suggest we need to conduct further analysis to distinguish the significant predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3e9d5-b439-4281-b0c6-6d750179b133",
   "metadata": {},
   "source": [
    "### Predictor variables selection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691828b9-fa4b-46a2-b7e4-61079cb43d7b",
   "metadata": {},
   "source": [
    "* We begin our data analysis by tuning our classifier to choose which variables from our data will be treated as predictor variables. \n",
    "* Next, we want to determine if any irrelevant predictors in our data set will negatively impact our classifier.\n",
    "* To ensure that the choice of which variables to include as predictors is part of tuning our classifier, we conduct this on our **training set**.\n",
    "* We will use the forward selection method to find the best predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58879dd8-e33f-4b39-9e1f-ab9e3a4dfcde",
   "metadata": {},
   "source": [
    "In forward selection, we need to\n",
    "1. start with a model having no predictors\n",
    "2. run the following three steps until you run out of predictors:\n",
    "\n",
    "    A.  for each unused predictor, add it to the model to form a candidate model\n",
    "    \n",
    "    B. tune all of the candidate models\n",
    "    \n",
    "    C. update the model to be the candidate model with the highest cross-validation accuracy\n",
    "3. select the model that provides the best trade-off between accuracy and simplicity\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f173f7-cad8-4e32-b34d-02a7d6dfeab5",
   "metadata": {},
   "source": [
    "**We will be selecting the 2 best predictors according to this method in our code so that it runs relatively faster than the code for 5 predictors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6c917-994a-43f5-b992-346944a110e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1) # Used to make the predictors selection reproducible\n",
    "\n",
    "\n",
    "uk_subset1 <- user_knowledge_training |> select(STG,SCG,STR,LPR,PEG,UNS )\n",
    "names1 <- colnames(uk_subset1 |> select(-UNS))\n",
    "\n",
    "# create an empty tibble to store the results\n",
    "accuracies <- tibble(size = integer(), model_string = character(), accuracy = numeric())\n",
    "\n",
    "# create a model specification\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "     set_engine(\"kknn\") |>\n",
    "     set_mode(\"classification\")\n",
    "\n",
    "# create a 5-fold cross-validation object\n",
    "uk_vfold <- vfold_cv(uk_subset1, v = 5, strata = UNS)\n",
    "\n",
    "# store the total number of predictors\n",
    "n_total <- length(names1)\n",
    "\n",
    "# stores selected predictors\n",
    "selected <- c()\n",
    "\n",
    "# for every size from 1 to the total number of predictors\n",
    "for (i in 1:n_total) {\n",
    "    # for every predictor still not added yet\n",
    "    accs <- list()\n",
    "    models <- list()\n",
    "    for (j in 1:length(names1)) {\n",
    "        # create a model string for this combination of predictors\n",
    "        preds_new <- c(selected, names1[[j]])\n",
    "        model_string <- paste(\"UNS\", \"~\", paste(preds_new, collapse=\"+\"))\n",
    "\n",
    "        # create a recipe from the model string\n",
    "        uk_recipe <- recipe(as.formula(model_string), data = uk_subset1) |>\n",
    "                          step_scale(all_predictors()) |>\n",
    "                          step_center(all_predictors())\n",
    "\n",
    "        # tune the KNN classifier with these predictors, and collect the accuracy for the best K\n",
    "        acc <- workflow() |>\n",
    "          add_recipe(uk_recipe) |>\n",
    "          add_model(knn_spec) |>\n",
    "          tune_grid(resamples = uk_vfold, grid = 10) |>\n",
    "          collect_metrics() |>\n",
    "          filter(.metric == \"accuracy\") |>\n",
    "          summarize(mx = max(mean))\n",
    "        acc <- acc$mx %>% unlist()\n",
    "\n",
    "        # add this result to the dataframe\n",
    "        accs[[j]] <- acc\n",
    "        models[[j]] <- model_string\n",
    "    }\n",
    "    jstar <- which.max(unlist(accs))\n",
    "    accuracies <- accuracies %>% add_row(size = i, model_string = models[[jstar]], accuracy = accs[[jstar]])\n",
    "    selected <- c(selected, names1[[jstar]])\n",
    "    names1 <- names1[-jstar]\n",
    "}\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f3310-da16-4383-babf-420e336113ea",
   "metadata": {},
   "source": [
    "* The table represents the models' respective accuracies for **ALL** predictor variables.\n",
    "* As meaningful predictors are added, the estimated accuracy increases substantially, whereas adding irrelevant variables causes the accuracy to either exhibit small fluctuations or decrease as the model attempts to tune the number of neighbors to account for the extra noise.\n",
    "* Naturally, we balance high accuracy with simplicity. Keeping this in mind, we find the best model by observing the point where the accuracy stops increasing dramatically and levels off or begins to decrease. Therefore, we chose **LPR** and **PEG** as our predictors since the accuracy decreases as we add more predictors to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbbdda-e35e-4628-981a-e08ffb0145b0",
   "metadata": {},
   "source": [
    "### Tuning our K-NN classifier\n",
    "\n",
    "* We now conduct the K-NN classification analysis on our  dataset with predictors: **LPR** and **PEG**\n",
    "* To do this, we:\n",
    "1. create a recipe with the predictors:**LPR** and **PEG**, specifying the data to be **user_knowledge_training**. Additionally, we did not center or scale our predictors since they were all already in the range of 0.0 to 1.0. When scaling is performed, the value/meaning behind the data is lost. So we decided that not scaling this data would be best.\n",
    "2. Define the specification for our k-nn model. We use `tune()` for the neighbours' argument because we will first tune our model to choose the best k-value for our model.\n",
    "3. Perform cross-validation with 5 folds. This is because our dataset is not too large, so 20% of the data for validation is considerably accurate.\n",
    "4. Create a workflow adding the recipe and the model and applying the `tune_grid()` function for cross-validation. We then collect the resulting accuracies with the `collect_metrics()` function.\n",
    "5. Finally, we plot the accuracies against the neighbors to visualize the most stable k nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038792e-b399-4561-be88-4b266e6232c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed (1) \n",
    "options(repr.plot.height = 8, repr.plot.width = 10)\n",
    "\n",
    "#Making the recipe with our predictor variables (LPR and PEG)\n",
    "user_recipe <- recipe(UNS ~ LPR + PEG , user_knowledge_training)\n",
    "               \n",
    "\n",
    "#Making the model using \"straight-line\" distance between points\n",
    "#We keep the number of neighbors as tune() in order to get the optimised k value usign cross-validation\n",
    "user_model <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "              set_engine(\"kknn\") |>\n",
    "              set_mode(\"classification\")\n",
    "\n",
    "#We use 5 folds for cross-validation as we beilieve that it would not require a lot of time\n",
    "user_vfold <- vfold_cv(user_knowledge_training, v = 5, strata = UNS)\n",
    "\n",
    "#We test 25 values of k\n",
    "klots <-  tibble(neighbors = seq(from = 1, to = 25, by = 1))\n",
    "\n",
    "#We put it all together in a workflow\n",
    "user_metrics <- workflow() |>\n",
    "                add_recipe(user_recipe) |> #adding the recipe\n",
    "                add_model(user_model) |> #adding the model\n",
    "                tune_grid(resamples = user_vfold, grid = klots) |> #using tune_grid as we use cross-validation\n",
    "                collect_metrics() #collecting the metrics of our cross validation\n",
    "\n",
    "#Using the metric fucntion to find the accuracy of our classifier\n",
    "user_accuracy <- user_metrics |> \n",
    "                 filter(.metric == \"accuracy\") #Filtering the column by \"accuracy\"\n",
    "\n",
    "#Plotting the k values (number of neighbors) and the corresponding accuracy to get an optimized k value\n",
    "cross_val_plot <- ggplot(user_accuracy, aes(x = neighbors, y = mean)) + \n",
    "                  geom_point() +\n",
    "                  geom_line() +\n",
    "                  geom_vline(xintercept = 6, colour = \"blue\")+\n",
    "                  labs(x = \"K Neighbors\", y = \"Accuracy\") +\n",
    "                  ggtitle(\" Accuracy vs Number of Neighbors\") +\n",
    "                  theme(text = element_text(size = 18))\n",
    "\n",
    "cross_val_plot\n",
    "\n",
    "\n",
    "# This visualization suggests that K=6 provides the highest and the most omptimal accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d403a43e-4d5a-4128-8dd6-e5d18619a9bf",
   "metadata": {},
   "source": [
    "* The above graph demonstrates that **k = 6** is the most optimal value. This is because it is relatively higher than most other neighbors and does not immediately fall as we increase or decrease the neighbor by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655258f-53ed-46ff-8331-cae744b2b0bf",
   "metadata": {},
   "source": [
    "###  The K-NN Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c120d46-2860-44ed-a11a-2b032b2bb21d",
   "metadata": {},
   "source": [
    "* Now, we use the number of neighbors as 6 to build our model and keep the recipe as before, as we still want to use the same two predictors. \n",
    "* We put it together in a workflow and then fit it into our **training dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7ac4e-b1a2-4194-887d-67a708675ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we know that k = 6 has the highest probability of being the highest accuracy classifier, we can make the model\n",
    "user_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 6) |>\n",
    "             set_engine(\"kknn\") |>\n",
    "             set_mode(\"classification\")\n",
    "\n",
    "user_fit <-  workflow() |>\n",
    "             add_recipe(user_recipe) |> #adding the recipe to the workflow\n",
    "             add_model(user_spec) |> #adding the model to the workflow\n",
    "             fit(data = user_knowledge_training) #fitting the workflow to our training data\n",
    "\n",
    "user_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91c87d2-b9f9-42f4-82f4-cd262fc95682",
   "metadata": {},
   "source": [
    "### Testing the accuracy of our K-NN classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b3fcc-19de-4fb1-b724-638b09be0459",
   "metadata": {},
   "source": [
    "* Now, we conduct our K-NN classification on the **testing set** with the help of our trained model.\n",
    "* We  use the `predict` function to predict the classes of the **testing set** and `bind_cols` to join the UNS class of the testing set to our predicted column to compare the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7562cee-ca28-41c3-ad16-4180de20afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the model set up, we can predict the UNS class\n",
    "user_predictions <- predict(user_fit, user_knowledge_test) |> \n",
    "                     bind_cols(user_knowledge_test) #adding the test data columns to our predicted column in order to see our classifier accuracy\n",
    "\n",
    "#We use the metrics functions on the table which contains the predicted column along with the actual column to see our classsifier accuracy\n",
    "user_metrics <- user_predictions |>\n",
    "                metrics(truth = UNS, estimate = .pred_class) |>\n",
    "                filter(.metric == \"accuracy\")\n",
    "\n",
    "user_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc47f80-44bc-4c51-8ba5-d7daa6b888aa",
   "metadata": {},
   "source": [
    "* It is observed in the table above that the accuracy of our predictions was extremely high at 99.2%.\n",
    "\n",
    " * To further analyse the classifier accuracy, we used `conf_mat` to make a confusion matrix. It helped us to see which observations had been predicted wrong and what were the correct classes of those observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf7f90-9e87-443b-af14-1d98ec4b726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make a confusion matrix to see which classes in particular have been predicted incorrectly and whether we can afford these errors (false negatives and/or postitives) or not\n",
    "user_conf <- user_predictions |>\n",
    "             conf_mat(truth = UNS, estimate = .pred_class)\n",
    "\n",
    "\n",
    "autoplot(user_conf, type = \"heatmap\") +\n",
    "    scale_fill_distiller(palette = \"BuGn\", name = \"Frequency\") + #color blind friendly\n",
    "    labs(title = \"Confusion Matrix\") +\n",
    "    theme(legend.position = \"right\", text = element_text(size = 20),\n",
    "          plot.caption = element_text(size = 20, hjust = 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a6adb-92b0-4fde-b330-261757bfb71c",
   "metadata": {},
   "source": [
    "* The confusion matrix shows that our classifier’s only wrong prediction was not terrible because the classifier predicted *“Low”* when the actual value was *‘Very Low’*, which is just one class level up.\n",
    "\n",
    "* Finally, we make an interference plot to visualize our classifier's accuracy by shading the graph's area with the color of the predicted class and coloring the data points with their correct category. This way, we can visually see which data points lie in the accurate shaded color (predicted correctly) and which do not (mispredicted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bb4f8-ee7e-4eb1-855d-378ddf3c66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a final plot of our classsifer which colours our classifier predictions by area\n",
    "\n",
    "# create the grid of LPR/PEG vals, and arrange in a data frame\n",
    "are_grid <- seq(min(User_Knowledge$LPR), \n",
    "                max(User_Knowledge$LPR), \n",
    "                length.out = 100)\n",
    "smo_grid <- seq(min(User_Knowledge$PEG), \n",
    "                max(User_Knowledge$PEG), \n",
    "                length.out = 100)\n",
    "asgrid <- as_tibble(expand.grid(LPR = are_grid, \n",
    "                                PEG = smo_grid))\n",
    "\n",
    "# use the fit workflow to make predictions at the grid points\n",
    "knnPredGrid <- predict(user_fit, asgrid)\n",
    "\n",
    "# bind the predictions as a new column with the grid points\n",
    "prediction_table <- bind_cols(knnPredGrid, asgrid) %>% \n",
    "                    rename(Class = .pred_class)\n",
    "\n",
    "#making the decided plot\n",
    "plot <- ggplot() +\n",
    "              geom_point(data = User_Knowledge, \n",
    "              mapping = aes(x = LPR, \n",
    "                            y = PEG,\n",
    "                            color = UNS), \n",
    "              alpha = 0.75) +\n",
    "              geom_point(data = prediction_table, \n",
    "              mapping = aes(x = LPR, \n",
    "                            y = PEG,\n",
    "                            color = Class), \n",
    "              alpha = 0.02, \n",
    "              size = 5) +\n",
    "              labs(color = \"UNS\", \n",
    "              x = \"LPR\", \n",
    "              y = \"PEG\") +\n",
    "              ggtitle(\"PEG vs LPR\") +\n",
    "              theme(text = element_text(size = 17))\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1298d-5a6b-4821-96b0-9726ee6058fd",
   "metadata": {},
   "source": [
    "* From the plot, we observe that there are barely a few points that lie outside their right color (the correct predicted color) which further emphasizes the excellent quality of our classifier. We also observe that as the values of both our predictor variables increases, the level of UNS class also increases (*Very Low* < *Low* < *Middle* < *High*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08b9ba-6fb4-42cf-998e-9a0cc7ffd4ff",
   "metadata": {},
   "source": [
    "## Discussion ##\n",
    "\n",
    "In this classification, we only used 2 predictors (LPR & PEG). As evident through our analysis above, our classifier accuracy is 99.2%. The significantly high accuracy has proved that our selection of predictors is correct. From the confusion matrix, we only got 1 prediction wrong. Overall, the matrix does not show significant false positives or negatives at all. Thus, we believe that the accuracy of our classifier is accurate to make any future predictions. <br>\n",
    "<br>\n",
    "The final plot visualizes all the predictions and it shows the positive relationship between exam performance (LPR & PEG)  and user knowledge level (UNS). This matches our expectations as we believe that the better the exam performance of students, the higher the overall knowledge. In conclusion, better exam performance almost always correlates with higher knowledge level of students in the course. <br>\n",
    "<br>\n",
    "On the other hand, we believe that the findings can contribute to the research area of education. In the future, researchers can further improve the quality of teaching and student learning experience by designing better study habits for students based on the conclusions of the findings. <br>\n",
    "<br>\n",
    "Some future questions this could lead to is:\n",
    "\n",
    "1. Whether students have a better learning experience by studying/ majoring in 2 related/ similar areas (e.g., Computer Science and Physics or English Literature and History) simultaneously.\n",
    "\n",
    "2. Does repetition study method help students achieve a higher knowledge level in the subjects? If so, what is the best number of times for students to study the exact same material in a week in order to achieve the best exam result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d5491-8eb0-4d57-82f5-e8dce7502b26",
   "metadata": {},
   "source": [
    "***Reference***\n",
    "\n",
    "1. UCI Machine Learning Repository: Data Sets. (n.d.). Retrieved December 1, 2022, from https://archive.ics.uci.edu/ml/datasets/User%2BKnowledge%2BModeling\n",
    "\n",
    "2. 1. H. T. Kahraman, Sagiroglu, S., Colak, I., Developing intuitive knowledge classifier and modeling of users' domain dependent data in web, Knowledge Based Systems, vol. 37, pp. 283-295, 2013."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
